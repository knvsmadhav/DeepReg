dataset:
  dir:
    train: "demos/unpaired_ct_abdomen/data/train"
    valid: "demos/unpaired_ct_abdomen/data/valid"
    test: "demos/unpaired_ct_abdomen/data/test"
  format: "nifti"
  type: "unpaired" # paired / unpaired / grouped
  labeled: true
  image_shape: [192, 160, 256]

train:
  # define neural network structure
  model:
    method: "ddf" # the registration method, value should be ddf / dvf / conditional
    backbone: "local" # value should be local / global / unet
    local:
      num_channel_initial: 1 # number of initial channel in local net, controls the size of the network
      extract_levels: [0, 1, 2, 3, 4]
    unet:
      num_channel_initial: 1 # number of initial channel in u-net, controls the size of the network
      depth: 2 # depth of u-net, the input is at depth = 0, and the bottom is at depth = depth
      pooling: true # for downsampling, use non-parameterized pooling if true, otherwise use conv3d
      concat_skip: false #ã€€when upsampling, concatenate skipped tensor if true, otherwise use addition

  # define the loss function for training
  loss:
    dissimilarity:
      image:
        name: "lncc"
        weight: 0.1
      label:
        weight: 1.0
        name: "multi_scale"
        multi_scale:
          loss_type: "dice"
          loss_scales: [0, 1, 2, 4, 8, 16, 32]
        single_scale:
          loss_type: "cross-entropy"
    regularization:
      weight: 0.5 # weight of regularization loss
      energy_type: "bending" # value should be bending / gradient-l1 / gradient-l2

  # define the optimizer
  optimizer:
    name: "adam" # value should be adam / sgd / rms
    adam:
      learning_rate: 1.0e-5

  # define the hyper-parameters for preprocessing
  preprocess:
    batch_size: 2
    shuffle_buffer_num_batch: 1 # shuffle_buffer_size = batch_size * shuffle_buffer_num_batch

  # other training hyper-parameters
  epochs: 2 # number of training epochs
  save_period: 2 # the model will be saved every `save_period` epochs.
